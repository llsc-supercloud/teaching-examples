{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Word Count: Overall Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When most people hear Map Reduce, they usually think Hadoop, Java, Big Data, etc. Map Reduce is actually a parallel programming model that has been around for a long time. It consists of two steps: a map step where an operation is executed on a number of files in parallel (like the throughput example we just talked about), and a reduce step where the output of the map step is combined into a single output.\n",
    "\n",
    "In our word count example, the top 5 overall word counts is a good example of a problem that fits the Map Reduce paradigm. Getting the word counts for each book is the Map step, and combining the those word counts to get an overall count across the corpus is the Reduce step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MapReduce.png\" alt=\"Map Reduce\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce with LLMapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run a MapReduce job with very few changes to your serial code. This approach is language-agnostic, although the specific implementation may change with language.\n",
    "\n",
    "LLMapReduce is a command that is available on LLSC Systems and the MIT Supercloud. It is a bit like running a Job Array for the map step, followed by a serial job for the reduce step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Point: Serial Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here is the starting serial Python code with Slurm Submission script. Since we would submit this with LLsub, it doesn't contain any Slurm options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Serial Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpers import *\n",
    "\n",
    "# Load the file names                                                                                                                                      \n",
    "dataDir = \"../books/\"\n",
    "fnames = os.listdir(dataDir)\n",
    "\n",
    "allcounts = []\n",
    "for fname in fnames:\n",
    "    f = open(dataDir+fname, 'r', encoding='utf-8')\n",
    "    text = cleantext(f.readlines())\n",
    "    allcounts.append(countwords(text))\n",
    "\n",
    "globalcounts = dict()\n",
    "for counts in allcounts:\n",
    "    globalcounts = { k: counts.get(k, 0) + globalcounts.get(k, 0) for k in set(counts) | set(globalcounts) }\n",
    "    \n",
    "top5 = sorted(globalcounts, key=globalcounts.get, reverse=True)[:5]\n",
    "for k in top5:\n",
    "    print(\"{}: {}\".format(k, globalcounts[k]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Submission Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize the Module command\n",
    "source /etc/profile\n",
    "\n",
    "# Load the anaconda module\n",
    "module load anaconda/2020a\n",
    "\n",
    "\n",
    "# Run your script as you would from the command line\n",
    "python top5overall.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Map and Reduce Portions of your Code and Separate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two separate files to put the Map and Reduce portions of your code. The Map step can usually be identified by some sort of for loop- something you could implement as a Job Array. The Reduce step should take the result of the Map step and produce a single result.\n",
    "\n",
    "In our example, the Map step is counting the number of times each word appears in each book. The reduce step is summing the counts for each word to get an overall count.\n",
    "\n",
    "We create two files: `top5overall_map.py` and `top5overall_reduce.py`.\n",
    "\n",
    "The contents of `top5overall_map.py`:\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpers import *\n",
    "\n",
    "# Load the file names                                                                                                                                      \n",
    "dataDir = \"../books/\"\n",
    "fnames = os.listdir(dataDir)\n",
    "\n",
    "allcounts = []\n",
    "for fname in fnames:\n",
    "    f = open(dataDir+fname, 'r', encoding='utf-8')\n",
    "    text = cleantext(f.readlines())\n",
    "    allcounts.append(countwords(text))\n",
    "```\n",
    "\n",
    "The contents of `top5overall_reduce.py`:\n",
    "```python\n",
    "globalcounts = dict()\n",
    "for counts in allcounts:\n",
    "    globalcounts = { k: counts.get(k, 0) + globalcounts.get(k, 0) for k in set(counts) | set(globalcounts) }\n",
    "    \n",
    "top5 = sorted(globalcounts, key=globalcounts.get, reverse=True)[:5]\n",
    "for k in top5:\n",
    "    print(\"{}: {}\".format(k, globalcounts[k]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the Map Script: Reading the Input File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMapReduce will pass in the name of a file containing the list of files an individual process should iterate over. You'll have to read in this file and edit your code to iterate over the line of it.\n",
    "\n",
    "To read it in:\n",
    "\n",
    "```python\n",
    "inOutFileList = open(sys.argv[1],\"r+\")\n",
    "```\n",
    "\n",
    "Then change the start of your for loop:\n",
    "\n",
    "```python\n",
    "for line in inOutFileList.readlines():\n",
    "\n",
    "    # Get input and output file names for this iteration\n",
    "    (inFile,outFile) = line.split()\n",
    "    print(\"Reading from \" + inFile + \" and writing to \" + outFile)\n",
    "\n",
    "    # Read in file and clean the text\n",
    "    infid  = open(inFile,\"r+\", encoding='utf-8')\n",
    "    text = cleantext(infid.readlines())\n",
    "    ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the Map Script: Writing the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to write the result out into a file at the end of each iteration. Since I'm writing a dictionary, I'm using a JSON file, but you could use any other file type that you can easily read and write:\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # Save the word counts to the passed in output file name \n",
    "    with open(outFile+'.json', 'w') as fp:\n",
    "        json.dump(wordCounts, fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the Reduce Script: Grab the Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMapReduce passes two arguments into the Reduce script:\n",
    "- The directory where the output of the Map step is written\n",
    "- The name of the file where the output of the Reduce step should be written\n",
    "\n",
    "By now you are probably familiar with grabbing arguments. You'll grab them and then read in all the input files:\n",
    "\n",
    "```python\n",
    "# Grab the two arguments that are passed in\n",
    "# The first is the directory containing the output of the mapper\n",
    "# The second is the name of the file where the final word counts should be saved\n",
    "inputdir = sys.argv[1]\n",
    "outputfile = sys.argv[2]\n",
    "\n",
    "# Read in the first word count file\n",
    "fnames = os.listdir(inputdir)\n",
    "with open(inputdir+\"/\"+fnames[1], 'r') as fp:\n",
    "    globalranks = json.load(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the Reduce Script: Writing the Final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can write out the final ouptut. I'm writing to a csv file because it is both human readable and easily parsed in case it needs to be read by another program.\n",
    "\n",
    "```python\n",
    "# Save to the output file\n",
    "with open(outputfile+'.csv', 'w') as output_file:\n",
    "    csv.writer(output_file).writerows({k: globalcounts[k] for k in top5}.items())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Map and Reduce Wrapper Bash Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to create wrapper bash scripts for the mapper and reducer. These are fairly standard and there isn't much to them, but this is where you can load any modules or set any environment variables you might need.\n",
    "\n",
    "For `mapper.sh`:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize Modules\n",
    "source /etc/profile\n",
    "\n",
    "# Load the anaconda module\n",
    "module load anaconda/2020a\n",
    "\n",
    "# Call your script as you would from the command line, passing in $1 and $2 as arugments\n",
    "# Note that $1 and $2 are the arguments passed into this script\n",
    "python top5overall_map.py $1 $2\n",
    "```\n",
    "\n",
    "For `reducer.sh`\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Initialize Modules\n",
    "source /etc/profile\n",
    "\n",
    "# Load the anaconda module\n",
    "module load anaconda/2020a\n",
    "\n",
    "# Call your script as you would from the command line, passing in $1 and $2 as arugments\n",
    "# Note that $1 and $2 are the arguments passed into this script\n",
    "python top5overall_reduce.py $1 $2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLMapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to call `LLMapReduce`:\n",
    "\n",
    "\n",
    "```bash\n",
    "LLMapReduce --mapper mapper.sh --reducer reducer.sh --input path/to/data/ --output counts --apptype=mimo --np=4\n",
    "```\n",
    "\n",
    "If you would like to keep the output and intermeidate files, say for debugging, you can add the option `--keep=true`. This will keep the log files that are written out.\n",
    "\n",
    "There are many options to `LLMapReduce`. You can see them with a short description by running `LLMapReduce -h` at the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Python Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top5overall_map.py`:\n",
    "```python\n",
    "import os, sys, json\n",
    "sys.path.append('../')\n",
    "from helpers import *\n",
    "\n",
    "inOutFileList = open(sys.argv[1],\"r+\")\n",
    "\n",
    "for line in inOutFileList.readlines():\n",
    "\n",
    "    # Get input and output file names for this iteration\n",
    "    (inFile,outFile) = line.split()\n",
    "    print(\"Reading from \" + inFile + \" and writing to \" + outFile)\n",
    "\n",
    "    # Read in file and clean the text\n",
    "    infid  = open(inFile,\"r+\", encoding='utf-8')\n",
    "    text = cleantext(infid.readlines())\n",
    "\n",
    "    # Count number of times each word appears\n",
    "    wordCounts =countwords(text)\n",
    "\n",
    "    # Save the word counts to the passed in output file name \n",
    "    with open(outFile+'.json', 'w') as fp:\n",
    "        json.dump(wordCounts, fp)\n",
    "```\n",
    "\n",
    "`top5overall_reduce.py`:\n",
    "```python\n",
    "import os, sys, json, csv\n",
    "sys.path.append('../')\n",
    "from helpers import *\n",
    "\n",
    "# Grab the two arguments that are passed in\n",
    "# The first is the directory containing the output of the mapper\n",
    "# The second is the name of the file where the final word counts should be saved\n",
    "inputdir = sys.argv[1]\n",
    "outputfile = sys.argv[2]\n",
    "\n",
    "# Read in the first word count file\n",
    "fnames = os.listdir(inputdir)\n",
    "with open(inputdir+\"/\"+fnames[1], 'r') as fp:\n",
    "    globalranks = json.load(fp)\n",
    "\n",
    "# Merge the counts from all files\n",
    "globalcounts = dict()\n",
    "for fname in fnames:\n",
    "    with open(inputdir+\"/\"+fname, 'r') as fp:\n",
    "        counts = json.load(fp)\n",
    "    globalcounts = { k: counts.get(k, 0) + globalcounts.get(k, 0) for k in set(counts) | set(globalcounts) }\n",
    "    \n",
    "# Sort and print the top 5 words with their counts\n",
    "top5 = sorted(globalcounts, key=globalcounts.get, reverse=True)[:5]\n",
    "for k in top5:\n",
    "    print(\"{}: {}\".format(k, globalcounts[k]))\n",
    "\n",
    "# Save to the output file\n",
    "with open(outputfile+'.csv', 'w') as output_file:\n",
    "    csv.writer(output_file).writerows({k: globalcounts[k] for k in top5}.items())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Wrapper Bash Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mapper.sh`:\n",
    "```bash\n",
    "#!/bin/sh\n",
    "\n",
    "# Initialize Modules\n",
    "source /etc/profile\n",
    "\n",
    "# Load the anaconda module\n",
    "module load anaconda/2020a\n",
    "\n",
    "# Call your script as you would from the command line, passing in $1 and $2 as arugments\n",
    "# Note that $1 and $2 are the arguments passed into this script\n",
    "python top5overall_map.py $1 $2\n",
    "```\n",
    "\n",
    "`reducer.sh`:\n",
    "```bash\n",
    "#!/bin/sh\n",
    "\n",
    "# Initialize Modules\n",
    "source /etc/profile\n",
    "\n",
    "# Load the anaconda module\n",
    "module load anaconda/2020a\n",
    "\n",
    "# Call your script as you would from the command line, passing in $1 and $2 as arugments\n",
    "# Note that $1 and $2 are the arguments passed into this script\n",
    "python top5overall_reduce.py $1 $2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
