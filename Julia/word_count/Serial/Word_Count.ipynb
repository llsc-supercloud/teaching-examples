{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[\"as\", \"a\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\",\n",
    "    \"after\", \"afterwards\", \"again\", \"against\", \"ain\", \"t\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\",\n",
    "    \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\",\n",
    "    \"appreciate\", \"appropriate\", \"are\", \"aren\", \"t\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\",\n",
    "    \"at\", \"available\", \"away\", \"awfully\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\",\n",
    "    \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\",\n",
    "    \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"c\", \"mon\", \"c\", \"s\", \"came\", \"can\",\n",
    "    \"can\", \"t\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\",\n",
    "    \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\",\n",
    "    \"contains\", \"corresponding\", \"could\", \"couldn\", \"t\", \"course\", \"currently\", \"definitely\", \"described\",\n",
    "    \"despite\", \"did\", \"didn\", \"t\", \"different\", \"do\", \"does\", \"doesn\", \"t\", \"doing\", \"don\", \"t\", \"done\", \"down\",\n",
    "    \"downwards\", \"during\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\",\n",
    "    \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n",
    "    \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \"followed\",\n",
    "    \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\",\n",
    "    \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\",\n",
    "    \"had\", \"hadn\", \"t\", \"happens\", \"hardly\", \"has\", \"hasn\", \"t\", \"have\", \"haven\", \"t\", \"having\", \"he\", \"he\", \"s\",\n",
    "    \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here\", \"s\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n",
    "    \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"d\",\n",
    "    \"i\", \"ll\", \"i\", \"m\", \"i\", \"ve\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\",\n",
    "    \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn\", \"t\", \"it\", \"it\", \"d\",\n",
    "    \"it\", \"ll\", \"it\", \"s\", \"its\", \"itself\", \"just\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"last\",\n",
    "    \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"let\", \"s\", \"like\", \"liked\",\n",
    "    \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\",\n",
    "    \"meanwhile\", \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\",\n",
    "    \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\",\n",
    "    \"novel\", \"now\", \"nowhere\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\",\n",
    "    \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"out\", \"outside\", \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\",\n",
    "    \"plus\", \"possible\", \"presumably\", \"probably\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\",\n",
    "    \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"said\", \"same\", \"saw\",\n",
    "    \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\",\n",
    "    \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \"should\",\n",
    "    \"shouldn\", \"t\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\",\n",
    "    \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\",\n",
    "    \"such\", \"sup\", \"sure\", \"t\", \"s\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\",\n",
    "    \"that\", \"that\", \"s\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "    \"there\", \"s\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"they\", \"d\",\n",
    "    \"they\", \"ll\", \"they\", \"re\", \"they\", \"ve\", \"thing\", \"things\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\",\n",
    "    \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\",\n",
    "    \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"un\", \"under\", \"unfortunately\", \"unless\",\n",
    "    \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\",\n",
    "    \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasn\", \"t\", \"way\", \"we\", \"we\", \"d\",\n",
    "    \"we\", \"ll\", \"we\", \"re\", \"we\", \"ve\", \"welcome\", \"well\", \"went\", \"were\", \"weren\", \"t\", \"what\", \"what\", \"s\", \"whatever\",\n",
    "    \"when\", \"whence\", \"whenever\", \"where\", \"where\", \"s\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "    \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who\", \"s\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "    \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won\", \"t\", \"wonder\", \"would\", \"would\",\n",
    "    \"wouldn\", \"t\", \"yes\", \"yet\", \"you\", \"you\", \"d\", \"you\", \"ll\", \"you\", \"re\", \"you\", \"ve\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \"zero\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getwordcounts (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cleantext(text)\n",
    "    \n",
    "    # Get full text without whitespace, punctuation, or stopwords\n",
    "    idx = findall(occursin.(\"***\",text))\n",
    "    text = text[idx[1]+1:idx[2]-1]\n",
    "    text = replace(join(text[text.!=\"\"],\" \"),r\"\\p{P}|[0-9]\" => \" \")\n",
    "    text = lowercase(replace(text, r\"\\s{2,}\" => \" \"))\n",
    "\n",
    "    for w in stopwords\n",
    "        text = replace(text,\" \"*w*\" \" => \" \");\n",
    "    end\n",
    "\n",
    "    return text\n",
    "end\n",
    "\n",
    "function countwords(text)\n",
    "    textSep = split(text,\" \")\n",
    "    uwords = unique(textSep)\n",
    "    \n",
    "    wordcounts = Dict{String,Float64}()\n",
    "    for w in textSep\n",
    "        wordcounts[w] = get(wordcounts, w, 0.0) + 1.0\n",
    "    end\n",
    "    \n",
    "    return wordcounts\n",
    "end\n",
    "\n",
    "function getwordcounts(fname)\n",
    "    # Read in file and clean the text\n",
    "    f = open(fname)\n",
    "    text=cleantext(readlines(f))\n",
    "\n",
    "    # Count number of times each word appears\n",
    "    return countwords(text)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Array{String,1}:\n",
       " \"../books/1013.txt.utf-8\"\n",
       " \"../books/1059-0.txt\"    \n",
       " \"../books/159.txt.utf-8\" \n",
       " \"../books/23218-0.txt\"   \n",
       " \"../books/35.txt.utf-8\"  \n",
       " \"../books/36.txt.utf-8\"  \n",
       " \"../books/5230.txt.utf-8\"\n",
       " \"../books/524-0.txt\"     \n",
       " \"../books/775-0.txt\"     \n",
       " \"../books/780-0.txt\"     \n",
       " \"../books/pg11696.txt\"   \n",
       " \"../books/pg31547.txt\"   \n",
       " \"../books/pg7308.txt\"    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLoc = \"../books/\"\n",
    "fnames = dataLoc.*readdir(dataLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 5 words for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair{String,Float64}[\"cavor\"=>302.0; \"moon\"=>212.0; \"time\"=>173.0; \"sphere\"=>127.0; \"made\"=>126.0]\n",
      "Pair{String,Float64}[\"world\"=>233.0; \"men\"=>178.0; \"king\"=>177.0; \"man\"=>176.0; \"time\"=>131.0]\n",
      "Pair{String,Float64}[\"man\"=>214.0; \"montgomery\"=>205.0; \"moreau\"=>144.0; \"beast\"=>109.0; \"men\"=>101.0]\n",
      "Pair{String,Float64}[\"room\"=>35.0; \"candle\"=>21.0; \"door\"=>18.0; \"man\"=>17.0; \"hand\"=>13.0]\n",
      "Pair{String,Float64}[\"time\"=>201.0; \"machine\"=>86.0; \"man\"=>70.0; \"traveller\"=>61.0; \"white\"=>59.0]\n",
      "Pair{String,Float64}[\"martians\"=>164.0; \"people\"=>159.0; \"man\"=>126.0; \"time\"=>122.0; \"black\"=>122.0]\n",
      "Pair{String,Float64}[\"man\"=>260.0; \"kemp\"=>245.0; \"mr\"=>223.0; \"invisible\"=>181.0; \"door\"=>171.0]\n",
      "Pair{String,Float64}[\"veronica\"=>731.0; \"ann\"=>715.0; \"life\"=>208.0; \"man\"=>194.0; \"love\"=>188.0]\n",
      "Pair{String,Float64}[\"graham\"=>580.0; \"man\"=>355.0; \"people\"=>275.0; \"men\"=>207.0; \"ostrog\"=>187.0]\n",
      "Pair{String,Float64}[\"bert\"=>616.0; \"time\"=>228.0; \"air\"=>206.0; \"man\"=>181.0; \"men\"=>171.0]\n",
      "Pair{String,Float64}[\"redwood\"=>325.0; \"bensington\"=>210.0; \"cossar\"=>174.0; \"food\"=>174.0; \"great\"=>161.0]\n",
      "Pair{String,Float64}[\"red\"=>102.0; \"slim\"=>71.0; \"industrialist\"=>42.0; \"astronomer\"=>41.0; \"animals\"=>30.0]\n",
      "Pair{String,Float64}[\"mr\"=>881.0; \"polly\"=>762.0; \"man\"=>156.0; \"uncle\"=>152.0; \"time\"=>141.0]\n",
      "  6.922606 seconds (5.08 M allocations: 2.352 GiB, 5.02% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time for i = 1:length(fnames)\n",
    "    \n",
    "    f = open(dataLoc*fnames[i])\n",
    "    text=cleantext(readlines(f))\n",
    "    \n",
    "    wordCounts = countwords(text)\n",
    "    \n",
    "    rankedwords = sort(collect(wordCounts), by=x->x[2], rev=true)\n",
    "\n",
    "    println(rankedwords[1:5,:])\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 5 words globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×1 Array{Pair{String,Float64},2}:\n",
       "    \"man\" => 1996.0\n",
       "   \"time\" => 1617.0\n",
       "     \"mr\" => 1527.0\n",
       " \"people\" => 1128.0\n",
       "  \"world\" => 1119.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get counts for each book\n",
    "allcounts = getwordcounts.(fnames)\n",
    "\n",
    "# Calculate the overall word counts\n",
    "overallcounts = merge(+,allcounts...)\n",
    "\n",
    "sort(collect(overallcounts), by=x->x[2], rev=true)[1:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 5 normalized word counts per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair{String,Float64}[\"cavorite\"=>1.0; \"mooncalf\"=>1.0; \"lunar\"=>1.0; \"cavor\"=>1.0; \"selenites\"=>1.0]\n",
      "Pair{String,Float64}[\"leblanc\"=>1.0; \"holsten\"=>1.0; \"fowler\"=>1.0; \"brissago\"=>1.0; \"karenin\"=>1.0]\n",
      "Pair{String,Float64}[\"montgomery\"=>1.0; \"puma\"=>1.0; \"moreau\"=>1.0; \"prendick\"=>1.0; \"swine\"=>0.964286]\n",
      "Pair{String,Float64}[\"withered\"=>0.692308; \"candles\"=>0.619048; \"candle\"=>0.567568; \"shade\"=>0.296296; \"room\"=>0.0686275]\n",
      "Pair{String,Float64}[\"psychologist\"=>1.0; \"morlocks\"=>1.0; \"weena\"=>1.0; \"filby\"=>1.0; \"sphinx\"=>1.0]\n",
      "Pair{String,Float64}[\"artilleryman\"=>1.0; \"ulla\"=>1.0; \"martian\"=>1.0; \"woking\"=>1.0; \"martians\"=>0.993939]\n",
      "Pair{String,Float64}[\"henfrey\"=>1.0; \"adye\"=>1.0; \"mariner\"=>1.0; \"griffin\"=>1.0; \"jaffers\"=>1.0]\n",
      "Pair{String,Float64}[\"vee\"=>1.0; \"ann\"=>1.0; \"alice\"=>1.0; \"miniver\"=>1.0; \"veronica\"=>1.0]\n",
      "Pair{String,Float64}[\"graham\"=>1.0; \"aeropile\"=>1.0; \"howard\"=>1.0; \"asano\"=>1.0; \"isbister\"=>1.0]\n",
      "Pair{String,Float64}[\"smallways\"=>1.0; \"edna\"=>1.0; \"germans\"=>1.0; \"bert\"=>1.0; \"butteridge\"=>1.0]\n",
      "Pair{String,Float64}[\"boomfood\"=>1.0; \"cossar\"=>1.0; \"hickleybrow\"=>1.0; \"wondershoot\"=>1.0; \"herakleophorbia\"=>1.0]\n",
      "Pair{String,Float64}[\"dad\"=>1.0; \"slim\"=>1.0; \"industrialist\"=>1.0; \"explorer\"=>0.956522; \"astronomer\"=>0.953488]\n",
      "Pair{String,Float64}[\"polly\"=>1.0; \"johnson\"=>1.0; \"rumbold\"=>1.0; \"fishbourne\"=>1.0; \"parsons\"=>1.0]\n"
     ]
    }
   ],
   "source": [
    "# Get counts for each book\n",
    "allcounts = getwordcounts.(fnames)\n",
    "\n",
    "# Calculate the overall word counts\n",
    "overallcounts = merge(+,allcounts...)\n",
    "\n",
    "# Iterate through each set of word counts\n",
    "for counts in allcounts\n",
    "\n",
    "    # Calculate normalized score\n",
    "    thresh = quantile(collect(values(counts)), .98)\n",
    "    counts = filter!(word-> word.second > thresh, counts)\n",
    "    normscore = merge(/,counts,filter(word->haskey(counts,word.first), overallcounts))\n",
    "\n",
    "    # Sort and print the top 5 words with their normalized counts\n",
    "    rankedwords = sort(collect(normscore), by=x->x[2], rev=true)\n",
    "    println(rankedwords[1:min(5,length(counts)),:])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
