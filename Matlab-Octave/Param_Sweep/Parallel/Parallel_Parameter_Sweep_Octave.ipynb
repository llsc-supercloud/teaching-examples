{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a parallel parameter sweep application from an existing serial version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebook assignment we created a serial parameter sweep application and ran the code from the Jupyter Notebook.  Using that code as a starting point we want to speed it up by distributing the work across multiple processing units.  Before we start there are a couple of concepts we need to consider.  Note that the subset defined below has been selected because the concepts directly affect the design of our parallel parameter sweep implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPMD v MPMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPMD stands for \"Single Program Multiple Data\".  The Single Program Multiple Data process means that a single program runs on all of the processors or cores but each process works on different data, e.g. each loop of a for loop includes the same calculations but operates on different indices or inputs. \n",
    "\n",
    "MPMD stands for \"Multiple Programs Multiple Data\".  In this workflow each process has its own compute task,  and its own data. In some cases the programming language choice is different between individual tasks. An example of a MPMD workflow would be a processing pipeline where each separate task in the pipeline is on a distinct processor, or sest of processors, such as the signal processing pipeline shown below. Note that in this case the processing can be divided by function or task.  For this reason, MPMD models are seen more frequently in Signal and Image Processing and Data Analysis applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory v Distributed Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In addition to understanding the different types of workflows, we need to consider the underlying architecture of the machine where the program will run.  Traditionally, machines were created with either shared or distributed memory.  More recently supercomputer architectures include \"hybrid\" memory, where the system is created from a number of shared memory nodes.  \n",
    "\n",
    "**Shared memory** systems are those where all of the processors can see the memory space.  Imagine that you are part of a team that is meeting in a conference room.  During the discussion people are taking notes on the white board.  Everyone in the room can access the white board, read it, write on it and erase it. In the computer system this is equivalent to each process being able to read from and write to the same memory space. \n",
    "\n",
    "**Distributed memory** systems are those where each machine is a separate island with its own processing cores and memory. Let's revisit the meeting example we introduced with shared memory.  Imagine that the project team is spread geographically.  In this case they are all remote from one another so everyone works in their own office on their own white board. In this case when you need to share information with another team member you need to send a message, or make a phone call, to your teammate's office.  Similarly, when someone else needs information they need to send a message to you or call you.  Furthermore, in order for this communication to happen, each team member needs to know the office locations, or phone numbers, of all the team members and their tasks. \n",
    "\n",
    "Distributed memory processing splits the processing between machines (generally referred to as nodes) that have their own local memory.  Each time the application code needs to send its results to another node or requires results processed on a different node, the information must be communicated or \"messaged\" between the two nodes.  As in the case of the team meetings, the nodes must understand which other nodes are involved in the compute job and which parts of the job they are processing. Understanding the locations of all the processors (meeting rooms) and what is happening in each is known as the **local-global mapping**.\n",
    "\n",
    "**Hybrid systems** In these systems, each node has many processors which can share the same memory.  To create a larger system, many shared memory nodes are combined, however, processors can only see the memory on their node, so messages must be sent between nodes. This model, the hybrid system, includes both shared and distributed memory systems. \n",
    "\n",
    "The analogous meeting example for hybrid memory is to split the full team meeting into a number of smaller team meetings each held in a separate  room.  All of the members of the small team can share the white board but if they need to get information to another team they have to send a message to the conference room where the other team is meeting.  Similarly, when they need information from another team, the other team needs to come to them.  As was the case with distributed computing, the communication requires that each team knows the locations of the other conference rooms and which team has been assigned to which conference room.\n",
    "\n",
    "Consider the meeting logistics for the three different meeting examples.  In the shared whiteboard case everyone in the room was able to talk and share with everyone else.  This meeting style didn't need anything special, just the team, the room and the whiteboard. In the case where the team was geographically \n",
    "\n",
    "**Programming Models** Each of the models; shared memory, distributed memory and hybrid memory has a slightly different programming model. For this example we will focus on distributed memory models.  In particular pMatlab/pOctave uses the \"Data Parallel Model\", also know as the \"Partitioned Global Address Space\" or PGAS model.  The PGAS model assumes a distributed memory architecture but provides an abstraction so that the memory is presented to the user as if it were one large shared global space. In this model, the user designs the algorithm from a global perspective, working on the application as if it were serial, and the model takes care of the details, including determining how the data is partitioned, which processor holds which data and how messages need to be passed.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstraction Through Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parallel Octave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Parallel Octave uses MIT Lincoln Laboratory's pMatlab library which is built on a Single Program Multiple Data, Distributed Memory, Data Parallel Model.  As a result, minimal code modifications are required by the application programmer in order to achieve reasonable speedup.\n",
    "\n",
    "As we work through the process of creating a parallel implementation of the parameter sweep application we will highlight the PGAS constructs and SPMD behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Introduction to the Map construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial to Parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand some of the models that we are working with it is time to review the serial code with an eye toward modifications that we need to make.  As part of the parallelization design we need to understand:\n",
    "\n",
    "* where the independence is, this will determine how we distributed the data\n",
    "* does the data need to be gathered up to be processed as a whole unit\n",
    "* which data structures need to be distributed\n",
    "\n",
    "Our goal in converting the serial code to parallel code is to parallelize the 'for' loop within the serial implementation. \n",
    "\n",
    "\n",
    "To start, let's consider the serial parameter sweep code, reviewing each code block in the serial implementation:\n",
    "\n",
    "* Do we need to modify the sample_function?\n",
    "* Do we need to modify the array sizes?\n",
    "\n",
    "When developing code you always want to have a way to test the correctness of the results. This is especially true when parallel versions of serial code as the addition of parallel processing can render the code difficult to debug.  One way to insure that you are able to recover the serial code from the parallel version is to include a switch that turns the parallel features on and off.  pMatlab/pOctave has been designed so that calls to parallel constructs result in a 'no op', in other words no operation is performed and the function returns.  \n",
    "\n",
    "In the next code cell, write the appropriate code for the sample_function and array sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When developing code you always want to have a way to test the correctness of the results. This is especially true when parallel versions of serial code as the addition of parallel processing can render the code difficult to debug.  One way to insure that you are able to recover the serial code from the parallel version is to include a switch that turns the parallel features on and off.  pMatlab/pOctave has been designed so that calls to parallel constructs result in a 'no op', in other words no operation is performed and the function returns.  \n",
    "\n",
    "Throughout this exercise we have provided some skeleton code to get you started, but in each step we use questions to guide the parallel code development.  \n",
    "\n",
    "In the next code cell, write the appropriate code to use the sample_function and set the array sizes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Block\n",
    "```Octave\n",
    "% basic parameter sweep code \n",
    "%\n",
    "% Goal: parallelize the following loop:\n",
    "% for ii = 1:n\n",
    "%   z(ii) = f(ii,...otherArguments)\n",
    "% end % for i loop\n",
    "\n",
    "% Turn parallelism on or off\n",
    "PARALLEL = 0;\n",
    "\n",
    "%addpath('/home/gridsan/jmullen/examples/Param_Sweep')\n",
    "\n",
    "% Set data sizes.\n",
    "m = 3; % number of output arguments\n",
    "n = 16; % number of independent iterations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not need to modify either the sample function or the array sizes.  This section of the code is identical to the serial version, we need to add the path to sample_function.m and set the array sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating the map that will distribute the data.  This step requires understanding the independence within the application.  As you look at the loop that we are parallelizing, which dimension of **z** is independent?\n",
    "\n",
    "Recall that the map construct in pMatlab/pOctave takes the following form:\n",
    "\n",
    "dataMap = map([row distribution column distribution], distributionType, processorList)\n",
    "\n",
    "For our application we will \n",
    " * use all the processors that we ask for at runtime \n",
    " * use a block distribution  \n",
    "\n",
    "With those criteria and the knowledge of the application, create the map in the next code cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Block\n",
    "```Octave\n",
    "\n",
    "% Create Maps\n",
    "map1 = 1;\n",
    "if PARALLEL\n",
    "% Distributed Map \n",
    "   map1 = map([Np 1], {}, 0:Np-1);\n",
    "end\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the distributed output matrix and getting the global indices\n",
    "\n",
    "In the serial version of the application, we looped over m rows and called the sample_function to insert data in n columns. In the parallel case the m rows will be distributed among Np processors, where each processor only knows about its portion of the array.  Locally each processor will run Octave on a subset of the full matrix looping rows from 1 to num_local.  \n",
    "\n",
    "Note that the sample_function hasn't changed, so we still expect the first column to hold the global row number for each local row and the 3rd column to hold a value 2.5 times the global row number.  \n",
    "\n",
    "\n",
    "In the next cell, we demonstrate how to \n",
    " * create a dmat, or distributed matrix\n",
    " \n",
    "and then for each processor we demonstrate how to \n",
    " * get the global indices (in this case the row numbers)\n",
    " * how to get a copy of the local portion of the matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Block\n",
    "```Octave\n",
    "\n",
    "% Create distributed array - dmat - \n",
    "% matrix is z - data output matrix\n",
    "z = zeros(n,m,map1);\n",
    "\n",
    "% Get the global indices that are assigned to my Processor ID  (Pid)\n",
    "my_i_global = global_ind(z,1);\n",
    "\n",
    "% Get a local copy of the portion of the array that is assigned to my Pid\n",
    "my_z = local(z);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "* Use of the map function\n",
    "  - the zeros and ones functions are overloaded so that the addition of \"map1\" in the calling statement results in the creation of a distributed matrix\n",
    "  - note that in the serial case map1 has been defined to be 1 so that the code doesn't error out during the creation of z\n",
    "  \n",
    "* Use of the global_ind function\n",
    "  - the global_ind function returns a vector with the global indices associated with the data that was assigned to a processor.  This is a SPMD program so each processor executes the same statement but receives a different set of indices.  \n",
    "\n",
    "* Use of the local function\n",
    "   - the local function is used to get a copy of the local array\n",
    "   - this call is not required but was added to optimize the local access to the array during computation\n",
    "   - to copy the updated local array back to the full dmat, the put_local command is used and will be demonstrated in a few more steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the for loop to enable parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the map, the distributed matrix z, computed the global indices and gotten a copy of the local portion of z, we are ready to create our parallel loop. \n",
    "\n",
    "Consider the for loop in this parameter sweep.  In serial the code loops over all of the rows and calls sample_function with 3 arguments: \n",
    "* loop index - row number in the serial case\n",
    "* processor ID number, pid\n",
    "* my_other_arg = 2.5 \\* (global row number)\n",
    "\n",
    "In the parallel case each process needs to loop over the rows assigned to the process via the dmat and\n",
    "* compute the global row number from the local index\n",
    "* know its pid (assigned as part of pMatlab/pOctave initialization)\n",
    "* compute my_other_arg using the global row number\n",
    "\n",
    "In our last code snippet we computed everything we needed to create the loop.  Let's look at how we compute the 3 arguments to sample_function.  \n",
    "1. Computing the loop indices\n",
    "  * my_i_global holds a vector of the global row numbers assigned to a processor\n",
    "  * the length of my_i_global is the length of the local matrix in the row dimension\n",
    "2. Computing global row number\n",
    "  * my_i_global holds a vector of the global row numbers assigned to a processor\n",
    "  * use the loop index as a pointer into my_i_global to get the global row number\n",
    "3. Pid is a reserved variable, initialized with pMatlab/pOctave init and each processor knows its Pid \n",
    "4. my_other_arg is straightforward once we have the global row number\n",
    "\n",
    "\n",
    "A final note about indexing before we look at the code:\n",
    "\n",
    "The goal is to view the application from a global perspective and use the pMatlab/pOctave constructs to ease the distribution, computation and collection of data. When calling the sample_function to compute and assign results to the local portion of the distributed array you need to remember to _Compute Globally, Assign Locally_.  You do this by using the local indices for assignment into arrays and global indices for calls to functions or computations.\n",
    "\n",
    "Note that this is importatnt because Matlab allows you to extend a matrix during runtime.  If you use global indices to assign your local matrix values, Matlab will extend the matrix to match the indices you are using. When you complete the for loop, the local matrix will not fit back into the distributed array and the code will fail with an error.\n",
    "\n",
    "Let's look at the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Block\n",
    "```Octave\n",
    "\n",
    "% Loop over the local indices, call sample_function and compute results for local portion of matrix\n",
    "for i_local = 1:length(my_i_global)\n",
    "   % use local index as pointer to my_i_global vector to get global index\n",
    "   i_global = my_i_global(i_local);\n",
    "\n",
    "   % calculate my_other_argument\n",
    "   my_other_arg = 2.5*i_global;\n",
    "   \n",
    "   % call sample function with global index, pid, my_other_arg and store result in row of local z matrix\n",
    "   my_z(i_local, :) = sample_function(i_global, pid, my_other_arg);\n",
    "end\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the data for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to produce a final result equivalent to our serial version, we need to gather the data into a single m x n matrix and display the results.  \n",
    "\n",
    "To gather the data we use a gather, or agg, function that collects the data from each processor onto the leader processor, pid = 0.  Prior to agging the data we copy the local data back into the global dmat.  \n",
    "\n",
    "Note that the final result of the agg command is a vanilla matlab/octave matrix on the leader processor, pid 0, and partial results on the remaining processes.  \n",
    "\n",
    "The code looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Code Block\n",
    "```Octave\n",
    "\n",
    "% Store the local portion of z (my_z) in the distributed matrix, z\n",
    "z = put_local(z, my_z);\n",
    "\n",
    "% Gather the data to the leader process using the agg function\n",
    "z_final = agg(z);\n",
    "\n",
    "% Declare success and display the results\n",
    "disp('SUCCESS');\n",
    "disp(z_final);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a pMatlab/pOctave Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel jobs require more setup than serial jobs because the system needs to know how many processors to allocate to a job and in some cases which type of processors.\n",
    "\n",
    "For pMatlab/pOctave jobs we use the command \n",
    "`eval(pRUN(argument1, argument2, argument3))`\n",
    "where the arguments are:\n",
    "* `argument1` = the name of the Octave script, without the extension, in quotes because it is a string. For example, 'myfile'.\n",
    "* `argument2` = the number of processors to use when running the job\n",
    "* `argument3` = the processors to use when running the job.  When running on TXE1 the argument is 'grid'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the parallel paramenter sweep code on 1,2, and 4 processors and collect the timing information.  The timing as implemented is very crude - how could it be improved?\n",
    "* Open a terminal window\n",
    "* Start up MATLAB `matlab -nodisplay -singleCompThread`\n",
    "* At the Octave/MATLAB command line type:\n",
    "`eval(pRUN('param_sweep_parallel_v2', #processors, 'grid'))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Adding Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One tenet of good software engineering is that programs should not be run on full scale inputs immediately.  Rather, programs should initially be run on a small test problem to verify functionality and to validate against known results.**\n",
    "\n",
    "The following is the recommended procedure for adding parallelism to a pMatlab application.  This procedure gradually adds complexity to running the application.\n",
    "\n",
    "1.\t**Run with 1 processor interactively (using LLsub -i) with the pMatlab library disabled.**  \n",
    " * This tests the basic serial functionality of the code.\n",
    " \n",
    "2.\t**Run with 1 processor interactively with pMatlab enabled.**  \n",
    " * Tests that the pMatlab library has not broken the basic functionality of the code.\n",
    " \n",
    "3.\t**Run with 2 processors on multiple machines.**\n",
    " * Test that the program works with network communication.\n",
    " \n",
    "4.\t**Run with 4 processors on multiple machines.**\n",
    "\n",
    "5.\t**Increase the number of processors, as desired.**\n",
    "\n",
    "_Note that \"More is not always better\" because there is a limit to the scalability of your application code. Finding the limit usually requires that you try varying numbers of processors until you see \"speed-down\", i.e. you reach a point where it takes longer with more processors._  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information on pMatlab/pOctave see: https://www.mit.edu/~kepner/pMatlab/\n",
    "\n",
    "For additional information on parallel programming models see: https://computing.llnl.gov/tutorials/parallel_comp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.1.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
